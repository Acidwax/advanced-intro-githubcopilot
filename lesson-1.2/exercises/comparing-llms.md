# Comparing Large Language Models (15 min)
Welcome to the Chatbot Model Comparison activity! Here, you will compare the output of three different chatbot models. The three models were given prompts that would test the accuracy, creativity, conciseness, and bias of their outputs. Your job is to select the model that performed best in each category. Let's get started!

Complete the activity [here](https://igfnaqfcyl-13589482-i.codehs.me/index.html).  Then edit this page and write down your reflections here:

### Which model did you find performed best overall, and why?
Model B was the best in my opinion as is gave concise information, with good small added details but not too much. Model A did not give enough information. Model C gave way too much information.

### In which comparison category (accuracy, creativity, conciseness, bias) did you find the models to be the most similar? What about the most different?
The most similar was the question that was entirely scientific and factual. The moon question. That is information that can be found easily and fact checked readily. Therefore all bots were able to provide the same details.

### Were you surprised by any of the results?
I was surprised by C's answer to the nurse question provided a biased response on a nurse being a woman between a certain age. It did back this information with a 91% of nurses are women, but not citing a source on statistics like that is not good, and appears biased.

### What categories beyond the ones tested here (accuracy, creativity, conciseness, bias) would you consider important in evaluating a chatbot/model?
You could test how it handles specific academic studies, how does it do with math questions? How does it do with english/literature questions? How does it do with science questions?
